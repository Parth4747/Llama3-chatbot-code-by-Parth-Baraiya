Build a Multiple PDF chat App with llama3 having a High speed Inference with Memory.

Important Note : Replace your groq api key in .env file first and then run with command.

1. You can direct use my code by typing the following command : chainlit run apppp.py
2. I added the new functionaliy of giving the question either by speaking or typing on the AI assistant in the app.14 file.
3. In that file you have to type P for speaking or direct write the question after scanning the file .
4. I additionally add new functionality of the diffrenciate a scanning and chating for adding the file for scanning at any time without ctarting the new chat in the app.py file .

<img width="950" alt="image" src="https://github.com/InsightEdge01/Multi-PDF-llama3Chat/assets/131486782/91f76aae-2d7f-45c9-b898-d6f25a64fd73">
